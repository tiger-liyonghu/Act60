#!/usr/bin/env python3
"""
03_scrape_bios.py â€” ä¸‹è½½é«˜ç®¡ç®€ä»‹ï¼Œä¸‰å±‚é˜²å¹»è§‰æ ¡éªŒ

ç”¨æ³•:
  python 03_scrape_bios.py HK
  python 03_scrape_bios.py SG
  python 03_scrape_bios.py ALL

æµç¨‹:
  leadership_url
    â†’ [Jina Reader ä¸‹è½½]          â† çŽ°æˆå·¥å…·ï¼Œä¸è‡ªå·±å†™çˆ¬è™«
    â†’ ä¿å­˜ raw_text åˆ° raw/        â† L1: ç‰©ç†ç•™æ¡£ï¼Œäº‹åŽå¯æº¯æº
    â†’ [LLM æå–ï¼Œtemperature=0]   â† ä¸¥æ ¼é€å­—å¤åˆ¶ï¼Œå¿…é¡»é™„ _source_sentence
    â†’ ç¨‹åºæ ¡éªŒ _source_sentence    â† L3: å­—ç¬¦ä¸²åŒ¹é…ï¼Œè‡ªåŠ¨æ‰“å‡
    â†’ Bio èµ„æ ¼æ£€æŸ¥                 â† èŒä½èŒƒå›´ + â‰¥2å¥ + å«èƒŒæ™¯ä¿¡æ¯
    â†’ è¾“å‡º scraped_{MARKET}.json   â† ä¾› 04_upload.py ç›´æŽ¥ä¸Šä¼ 

é˜²å¹»è§‰ä¸‰å±‚:
  L1  ä¿å­˜ raw_text åŽŸå§‹å…¨æ–‡ï¼ˆå¯éšæ—¶å›žæº¯ï¼‰
  L2  LLM Prompt å¼ºåˆ¶è¦æ±‚ _source_sentenceï¼ˆåŽŸæ–‡å¯¹åº”å¥ï¼‰
  L3  ç¨‹åºè‡ªåŠ¨æ ¡éªŒï¼š_source_sentence[:60] in raw_text â†’ ä¸é€šè¿‡åˆ™ bio=null

Bio åˆ¤æ–­æ ‡å‡†ï¼ˆç”¨æˆ·ç¡®è®¤ï¼‰:
  - èŒä½: C-suite + VP åŠä»¥ä¸Šï¼ˆè§ config.BIO_CRITERIAï¼‰
  - â‰¥ 2 å¥è¯
  - å«èŒä¸šèƒŒæ™¯å…³é”®è¯ï¼ˆexperience / joined / previously ç­‰ï¼‰

è¾“å…¥: data/leadership_urls_{MARKET}.json
è¾“å‡º: data/scraped_{MARKET}.json
      data/review_{MARKET}.csv   â† äººå·¥æ ¸éªŒå·¥ä½œè¡¨
"""

import sys
import json
import csv
import re
import time
import requests
from datetime import datetime, timezone
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from config import (
    JINA_BASE_URL, JINA_API_KEY,
    LLM_API_URL, LLM_API_KEY, LLM_MODEL,
    DATA_DIR, RAW_DIR, BIO_CRITERIA, MARKETS,
)

TODAY = datetime.now(timezone.utc).strftime("%Y%m%d")
SCRAPED_AT = datetime.now(timezone.utc).isoformat()


# ===================== Jina ä¸‹è½½ =====================

def jina_fetch(url: str) -> str:
    """Jina Reader ä¸‹è½½é¡µé¢ï¼Œè¿”å›ž markdown æ–‡æœ¬ã€‚"""
    jina_url = JINA_BASE_URL + url
    headers = {"Accept": "text/plain"}
    if JINA_API_KEY:
        headers["Authorization"] = f"Bearer {JINA_API_KEY}"
    resp = requests.get(jina_url, headers=headers, timeout=90)
    resp.raise_for_status()
    return resp.text


# ===================== LLM è°ƒç”¨ =====================

def llm_call(prompt: str) -> str:
    """DeepSeek / OpenAI å…¼å®¹æŽ¥å£ï¼Œtemperature=0 ç¡®ä¿æ— éšæœºæ€§ã€‚"""
    if not LLM_API_KEY:
        raise ValueError("è¯·è®¾ç½®çŽ¯å¢ƒå˜é‡ LLM_API_KEY")
    resp = requests.post(
        f"{LLM_API_URL}/chat/completions",
        headers={
            "Authorization": f"Bearer {LLM_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": LLM_MODEL,
            "temperature": 0,  # å…³é—­éšæœºæ€§
            "messages": [{"role": "user", "content": prompt}],
        },
        timeout=180,
    )
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


def clean_json_response(text: str) -> str:
    text = text.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        text = "\n".join(lines[1:])
        if text.endswith("```"):
            text = text[:-3]
    return text.strip()


# ===================== LLM Prompt =====================
# å…³é”®çº¦æŸï¼š
# - åªå¤åˆ¶åŽŸæ–‡ï¼Œä¸æ”¹å†™
# - å¿…é¡»æä¾› _source_sentenceï¼ˆç¨‹åºè¦ç”¨å®ƒåš L3 æ ¡éªŒï¼‰
# - temperature=0 å·²åœ¨ API å±‚è®¾ç½®

EXTRACT_BIOS_PROMPT = """ä½ æ˜¯æ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»Žä»¥ä¸‹ä¿é™©å…¬å¸é¢†å¯¼å±‚é¡µé¢åŽŸæ–‡ä¸­æå–é«˜ç®¡ä¿¡æ¯ã€‚

ã€æœ€ä¸¥æ ¼è§„åˆ™ â€” è¿ååˆ™è¾“å‡ºè¢«ç¨‹åºè‡ªåŠ¨ä¸¢å¼ƒã€‘
1. æ‰€æœ‰å­—æ®µåªèƒ½é€å­—å¤åˆ¶é¡µé¢åŽŸæ–‡ï¼Œä¸æŽ¨æ–­ã€ä¸è¡¥å……ã€ä¸ç¿»è¯‘ã€ä¸æ€»ç»“
2. bio_verbatim å¿…é¡»æ˜¯é¡µé¢åŽŸæ–‡ä¸­å…³äºŽè¯¥é«˜ç®¡çš„å®Œæ•´æè¿°ï¼Œä¸€å­—ä¸æ”¹
3. _source_sentence å¿…é¡»æ˜¯ bio_verbatim å†…å®¹åœ¨åŽŸæ–‡ä¸­å‡ºçŽ°çš„å®Œæ•´å¥å­
   ï¼ˆç¨‹åºä¼šè‡ªåŠ¨æ£€æŸ¥ï¼š_source_sentence å‰60å­—æ˜¯å¦å‡ºçŽ°åœ¨åŽŸæ–‡ä¸­ï¼‰
4. æ‰¾ä¸åˆ°æŸå­—æ®µåˆ™è¿”å›ž nullï¼Œç»å¯¹ä¸çŒœæµ‹æˆ–è¡¥å……
5. ä¸æ·»åŠ ä»»ä½•ä½ è‡ªå·±çš„çŸ¥è¯†ï¼Œå“ªæ€•ä½ çŸ¥é“è¯¥äººç‰©çš„å…¶ä»–ä¿¡æ¯

å…¬å¸: {company}
åœ°åŒº: {market}
æ¥æº URL: {source_url}

é¡µé¢åŽŸæ–‡:
{raw_text}

è¯·è¿”å›ž JSON æ•°ç»„ï¼ŒåŒ…å«é¡µé¢ä¸­å‡ºçŽ°çš„æ‰€æœ‰é«˜ç®¡ï¼š
[
  {{
    "name": "é«˜ç®¡è‹±æ–‡å…¨åï¼ˆåŽŸæ–‡ï¼‰",
    "name_zh": "ä¸­æ–‡åï¼ˆåŽŸæ–‡ä¸­æœ‰åˆ™å¡«ï¼Œå¦åˆ™ nullï¼‰",
    "title": "èŒä½åŽŸæ–‡ï¼ˆå®Œæ•´ï¼Œä¸ç¼©å†™ï¼‰",
    "bio_verbatim": "ç®€ä»‹åŽŸæ–‡ï¼Œé€å­—å¤åˆ¶ï¼Œä¸€å­—ä¸æ”¹",
    "_source_sentence": "bio å†…å®¹åœ¨åŽŸæ–‡ä¸­å¯¹åº”çš„å®Œæ•´å¥å­ï¼ˆä¾›ç¨‹åºæ ¡éªŒï¼Œå¿…é¡»èƒ½åœ¨åŽŸæ–‡ä¸­æ‰¾åˆ°ï¼‰"
  }}
]

åªè¿”å›ž JSON æ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ã€‚é¡µé¢ä¸­æ— é«˜ç®¡ä¿¡æ¯åˆ™è¿”å›ž []ã€‚"""


# ===================== L3 æ ¡éªŒï¼šå­—ç¬¦ä¸²åŒ¹é… =====================

def validate_source_in_raw(source_sentence: str, raw_text: str) -> bool:
    """
    L3 æ ¡éªŒï¼šæ£€æŸ¥ _source_sentence å‰60å­—ç¬¦æ˜¯å¦å‡ºçŽ°åœ¨ raw_text ä¸­ã€‚
    å–å‰60å­—è€Œéžå…¨å¥ï¼Œé¿å…æœ«å°¾ç©ºç™½å·®å¼‚å¯¼è‡´è¯¯åˆ¤ã€‚
    """
    if not source_sentence or not raw_text:
        return False
    # è§„èŒƒåŒ–ç©ºç™½åŽæ¯”è¾ƒ
    norm_raw = " ".join(raw_text.split())
    norm_src = " ".join(source_sentence.split())
    search_key = norm_src[:60] if len(norm_src) >= 60 else norm_src
    return bool(search_key) and search_key in norm_raw


# ===================== Bio èµ„æ ¼æ£€æŸ¥ =====================

def count_sentences(text: str) -> int:
    """ä¼°ç®—å¥æ•°ï¼ˆæŒ‰ . ! ? ã€‚ï¼ï¼Ÿ åˆ†å‰²ï¼‰ã€‚"""
    if not text:
        return 0
    parts = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    return sum(1 for p in parts if p.strip())


def has_background_info(bio: str) -> bool:
    """æ£€æŸ¥ bio æ˜¯å¦å«èŒä¸šèƒŒæ™¯å…³é”®è¯ã€‚"""
    if not bio:
        return False
    bio_lower = bio.lower()
    return any(kw in bio_lower for kw in BIO_CRITERIA["background_keywords"])


def is_title_in_scope(title: str) -> bool:
    """
    æ£€æŸ¥èŒä½æ˜¯å¦åœ¨é‡‡é›†èŒƒå›´å†…ã€‚
    æŽ’é™¤é¡¹ä¼˜å…ˆï¼Œå†åˆ¤æ–­çº³å…¥é¡¹ã€‚
    """
    if not title:
        return False
    title_lower = title.lower()
    if any(ex in title_lower for ex in BIO_CRITERIA["excluded_titles"]):
        return False
    return any(inc in title_lower for inc in BIO_CRITERIA["included_titles"])


def run_bio_checks(exec_data: dict, raw_text: str) -> dict:
    """
    å¯¹å•æ¡é«˜ç®¡è®°å½•æ‰§è¡Œå…¨éƒ¨æ ¡éªŒï¼Œè¿”å›žå«æ£€æŸ¥ç»“æžœçš„è®°å½•ã€‚
    verified_auto=True è¡¨ç¤ºé€šè¿‡æ‰€æœ‰ç¨‹åºæ ¡éªŒã€‚
    """
    bio = exec_data.get("bio_verbatim") or ""
    source_sentence = exec_data.get("_source_sentence") or ""
    title = exec_data.get("title") or ""

    checks = {
        # L3: _source_sentence å¿…é¡»å‡ºçŽ°åœ¨åŽŸæ–‡ä¸­ï¼ˆé˜²å¹»è§‰æ ¸å¿ƒï¼‰
        "source_in_raw":   validate_source_in_raw(source_sentence, raw_text),
        # Bio èµ„æ ¼
        "title_in_scope":  is_title_in_scope(title),
        "min_sentences":   count_sentences(bio) >= BIO_CRITERIA["min_sentences"],
        "has_background":  has_background_info(bio),
    }

    all_pass = all(checks.values())

    return {
        **exec_data,
        "verified_auto":  all_pass,
        "check_details":  checks,
        # L3 æ ¡éªŒå¤±è´¥ â†’ bio ç½® nullï¼Œä¸å†™å…¥æ•°æ®åº“
        "bio_verbatim":   bio if checks["source_in_raw"] else None,
    }


# ===================== ä¸»æµç¨‹ =====================

def process_market(market_code: str):
    urls_file = DATA_DIR / f"leadership_urls_{market_code}.json"
    if not urls_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° {urls_file}ï¼Œè¯·å…ˆè¿è¡Œ 02_find_leadership.py")
        return

    companies = json.loads(urls_file.read_text(encoding="utf-8"))
    with_url = [c for c in companies if c.get("leadership_url")]

    print(f"\n{'='*60}")
    print(f"ðŸ“ å¸‚åœº: {market_code} â€” {len(with_url)}/{len(companies)} å®¶æœ‰é¢†å¯¼å±‚é¡µé¢")
    print(f"{'='*60}")

    all_results = []

    for i, company in enumerate(with_url, 1):
        name = company["company_name"]
        url  = company["leadership_url"]
        print(f"\n[{i}/{len(with_url)}] {name}")
        print(f"  URL: {url}")

        # â”€â”€ L1: Jina ä¸‹è½½å¹¶ä¿å­˜åŽŸå§‹æ–‡æœ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            raw_text = jina_fetch(url)
        except Exception as e:
            print(f"  âŒ Jina ä¸‹è½½å¤±è´¥: {e}")
            continue

        slug = re.sub(r'[^A-Za-z0-9]', '_', name)[:40]
        raw_file = RAW_DIR / f"{market_code}_{slug}_{TODAY}.txt"
        raw_file.write_text(raw_text, encoding="utf-8")
        print(f"  [L1] åŽŸæ–‡å·²ä¿å­˜ â†’ {raw_file.name} ({len(raw_text):,} chars)")

        # â”€â”€ LLM æå–ï¼ˆL2: å¼ºåˆ¶è¦æ±‚ _source_sentenceï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        prompt = EXTRACT_BIOS_PROMPT.format(
            company=name,
            market=market_code,
            source_url=url,
            raw_text=raw_text[:14000],   # çº¦ 3500 tokensï¼Œç•™ä½™é‡
        )
        try:
            llm_resp = llm_call(prompt)
            cleaned = clean_json_response(llm_resp)
            executives = json.loads(cleaned)
        except Exception as e:
            print(f"  âŒ LLM è§£æžå¤±è´¥: {e}")
            continue

        print(f"  [LLM] æå–åˆ° {len(executives)} åé«˜ç®¡")

        # â”€â”€ L3 æ ¡éªŒ + Bio èµ„æ ¼æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for exec_data in executives:
            result = run_bio_checks(exec_data, raw_text)
            result.update({
                "company":     name,
                "company_zh":  company.get("company_name_zh"),
                "market":      market_code,
                "source_url":  url,
                "raw_file":    raw_file.name,
                "scraped_at":  SCRAPED_AT,
            })
            all_results.append(result)

            checks = result["check_details"]
            icon = "âœ…" if result["verified_auto"] else "âš ï¸ "
            print(f"  {icon} {result.get('name','?')} | {(result.get('title') or '')[:50]}")
            if not result["verified_auto"]:
                failed = [k for k, v in checks.items() if not v]
                print(f"     æ ¡éªŒæœªé€šè¿‡: {failed}")

        time.sleep(2)

    # â”€â”€ è¾“å‡º JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    out_json = DATA_DIR / f"scraped_{market_code}.json"
    out_json.write_text(json.dumps(all_results, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"\nðŸ“„ JSON â†’ {out_json.name}")

    # â”€â”€ è¾“å‡ºæ ¡éªŒæ—¥å¿— CSVï¼ˆä»…ä¾›å®¡è®¡å‚è€ƒï¼Œæ— éœ€æ“ä½œï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    out_csv = DATA_DIR / f"review_{market_code}.csv"
    fieldnames = [
        "å§“å", "å…¬å¸", "èŒä½",
        "bioæ‘˜è¦ï¼ˆå‰120å­—ï¼‰",
        "âœ“èŒä½åœ¨èŒƒå›´", "âœ“å¥æ•°â‰¥2", "âœ“å«èƒŒæ™¯ä¿¡æ¯", "âœ“åŽŸæ–‡å¥æ ¡éªŒ(L3)",
        "æ¥æºURL",
        "åŽŸæ–‡å¥å­ï¼ˆ_source_sentenceï¼‰",
        "rawæ–‡ä»¶",
        "ç¨‹åºè‡ªåŠ¨é€šè¿‡",
        "äººå·¥æ ¸éªŒ(Y/N/DEL)",
        "å¤‡æ³¨",
    ]
    with out_csv.open("w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in all_results:
            checks = r.get("check_details", {})
            bio = r.get("bio_verbatim") or ""
            writer.writerow({
                "å§“å":                    r.get("name", ""),
                "å…¬å¸":                    r.get("company", ""),
                "èŒä½":                    r.get("title", ""),
                "bioæ‘˜è¦ï¼ˆå‰120å­—ï¼‰":       bio[:120],
                "âœ“èŒä½åœ¨èŒƒå›´":             "Y" if checks.get("title_in_scope") else "N",
                "âœ“å¥æ•°â‰¥2":                "Y" if checks.get("min_sentences") else "N",
                "âœ“å«èƒŒæ™¯ä¿¡æ¯":             "Y" if checks.get("has_background") else "N",
                "âœ“åŽŸæ–‡å¥æ ¡éªŒ(L3)":         "Y" if checks.get("source_in_raw") else "N",
                "æ¥æºURL":                 r.get("source_url", ""),
                "åŽŸæ–‡å¥å­ï¼ˆ_source_sentenceï¼‰": r.get("_source_sentence", ""),
                "rawæ–‡ä»¶":                 r.get("raw_file", ""),
                "ç¨‹åºè‡ªåŠ¨é€šè¿‡":             "Y" if r.get("verified_auto") else "N",
                "äººå·¥æ ¸éªŒ(Y/N/DEL)":       "",   # â† äººå·¥å¡«å†™
                "å¤‡æ³¨":                    "",
            })
    print(f"ðŸ“‹ CSV  â†’ {out_csv.name}")

    auto_pass = sum(1 for r in all_results if r["verified_auto"])
    print(f"\nç¨‹åºè‡ªåŠ¨é€šè¿‡: {auto_pass}/{len(all_results)}")
    print(f"æ ¡éªŒæœªé€šè¿‡ï¼ˆbioå·²ç½®nullï¼‰: {len(all_results) - auto_pass}")
    print(f"\nä¸‹ä¸€æ­¥: python 04_upload.py {market_code}")
    print(f"ï¼ˆä»… verified_auto=True çš„è®°å½•ä¼šä¸Šä¼ ï¼Œæ ¡éªŒæœªé€šè¿‡çš„è®°å½•è·³è¿‡ï¼‰")


def main():
    market_arg = sys.argv[1].upper() if len(sys.argv) > 1 else "ALL"

    if market_arg == "ALL":
        for code in MARKETS:
            process_market(code)
            time.sleep(5)
    else:
        process_market(market_arg)


if __name__ == "__main__":
    main()
